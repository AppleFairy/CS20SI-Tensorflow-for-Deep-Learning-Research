{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to manage your experiments in TensorFlow\n",
    "## tf.train.Saver()\n",
    "\n",
    "-----\n",
    "A good practice is to periodically save the model’s parameters after a certain number of steps so that we can restore/retrain our model from that step if need be. The tf.train.Saver() class allows us to do so by saving the graph’s variables in binary files.\n",
    "```python\n",
    "tf.train.Saver.save(sess,\n",
    "                    save_path, \n",
    "                    global_step=None,\n",
    "                    latest_filename=None,\n",
    "                    meta_graph_suffix='meta',\n",
    "                    write_meta_graph=True,\n",
    "                    write_state=True)\n",
    "\n",
    "# define model\n",
    "# create a saver object\n",
    "saver = tf.train.Saver()\n",
    "# launch a session to compute the graph\n",
    "with tf.Session() as sess:\n",
    "    # actual training loop\n",
    "    for step in range(training_steps):\n",
    "        sess.run([optimizer])\n",
    "        \n",
    "        if (step + 1) % 1000==0:\n",
    "            saver.save(sess, 'checkpoint_directory/model\n",
    "```\n",
    "In TensorFlow lingo, the step at which you save your graph’s variables is called a checkpoint.\n",
    "Since we will be creating many checkpoints, it’s helpful to append the number of training steps our model has gone through in a variable called global_step. It’s a very common variable to see in TensorFlow program. We first need to create it, initialize it to 0 and set it to be not trainable, since we don’t want to TensorFlow to optimize it.\n",
    "```python\n",
    "self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "```\n",
    "We need to pass global_step as a parameter to the optimizer so it knows to increment global_step by one with each training step:\n",
    "```python\n",
    "self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss,\n",
    " global_step=self.global_step)\n",
    " ```\n",
    "To save the session’s variables in the folder ‘checkpoints’ with name model-name-global-step, we use this:\n",
    "```python\n",
    "saver.save(sess, 'checkpoints/skip-gram', global_step=model.global_step)\n",
    "```\n",
    "To restore the variables, we use tf.train.Saver.restore(sess, save_path). For example, you want to restore the checkpoint at 10,000th step.\n",
    "```python\n",
    "saver.restore(sess, 'checkpoints/skip-gram-10000')\n",
    "```\n",
    "But of course, we can only load saved variables if there is a valid checkpoint. What you probably want to do is that if there is a checkpoint, restore it. If there isn’t, train from the start. TensorFlow allows you to get checkpoint from a directory with tf.train.get_checkpoint_state(‘directory-name’). The code for checking looks something like this:\n",
    "```python\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "   saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"./graph/word2vec\\\\skip-gram.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"./graph/word2vec\\\\skip-gram.ckpt-1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "print(tf.train.get_checkpoint_state(os.path.dirname(\"./graph/word2vec/checkpoint\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, saver.save() stores all variables of the graph, and this is recommended. However, you can also choose what variables to store by passing them in as a list or a dict when we create the saver object. Example from TensorFlow documentation.\n",
    "```python\n",
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "# pass the variables as a dict:\n",
    "saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
    "# pass them as a list\n",
    "saver = tf.train.Saver([v1, v2])\n",
    "# passing a list is equivalent to passing a dict with the variable op names \n",
    "# as keys\n",
    "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
    "```\n",
    "Note that savers only save variables, not the entire graph, so we still have to create the graph ourselves, and then load in variables. The checkpoints specify the way to map from variable names to tensors.\n",
    "## tf.summary\n",
    "----\n",
    "We’ve been using matplotlib to visualize our losses and accuracy, which is cool but unnecessary because TensorBoard provides us with a great set of tools to visualize our summary statistics during our training. Some popular statistics to visualize is loss, average loss, accuracy. You can visualize them as scalar plots, histograms, or even images. So we have a new namescope in our graph to hold all the summary ops.\n",
    "```python\n",
    "def _create_summaries(self):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        tf.summary.scalar(\"loss\", self.loss\n",
    "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "        # because you have several summaries, we should merge them all\n",
    "        # into one op to make it easier to manage\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "```\n",
    "Because it’s an op, you have to execute it with sess.run()\n",
    "```python\n",
    "loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "```\n",
    "Now you’ve obtained the summary, you need to write the summary to file using the same FileWriter object we created to visual our graph.\n",
    "```python\n",
    "writer.add_summary(summary, global_step=step)\n",
    "```\n",
    "Now, if you go run tensorboard and go to http://localhost:6006/, in the Scalars page, you will see the plot of your scalar summaries. your loss in scalar plot.\n",
    "And the loss in histogram plot.\n",
    "If you save your summaries into different sub-folder in your graph folder, you can compare your progresses. For example, the first time we run our model with\n",
    "learning rate 1.0, we save it in ‘improved_graph/lr1.0’ and the second time we run our model, we save it in ‘improved_graph/lr0.5’, on the left corner\n",
    "of the Scalars page, we can toggle the plots of these two runs to compare them. This can be really helpful when you want to compare the progress made\n",
    "with different optimizers or different parameters.\n",
    "You can write a Python script to automate the naming of folders where you store the graphs/plots of each experiment. You can visualize the statistics as images using tf.summary.image.\n",
    "```python\n",
    "tf.summary.image(name, tensor, max_outputs=3, collections=None)\n",
    "```\n",
    "## Control randomization\n",
    "----\n",
    "I never realized what an oxymoron this sounds like until I’ve written it down, but the truth is that you often have to control the randomization process to get stable results for your experiments. You’re probably familiar with random seed and random state from NumPy. TensorFlow doesn’t allow to you to get random state the way numpy does (at least not that I know of -- I will double check), but it does allow you to get stable results in randomization through two ways:\n",
    "### Set random seed at operation level. \n",
    "All random tensors allow you to pass in seed value in their initialization. For example:\n",
    "```python\n",
    "my_var = tf.Variable(tf.truncated_normal((-1.0,1.0), stddev=0.1, seed=0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0421\n",
      "-4.85772\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "    print(sess.run(c)) # >> -5.97319"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, session is the thing that keeps track of random state, so each new session will start the random state all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0421\n",
      "1.0421\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With operation level random seed, each op keeps its own seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0421\n",
      "1.0421\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "d = tf.random_uniform([], -10, 10, seed=2)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "    print(sess.run(d)) # >> 3.57493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set random seed at graph level with tf.Graph.seed\n",
    "```python\n",
    "tf.set_random_seed(seed)\n",
    "```\n",
    "If you don’t care about the randomization for each op inside the graph, but just want to be able to replicate result on another graph (so that other people can replicate your results on their own graph), you can use tf.set_random_seed instead. Setting the current TensorFlow random seed affects the current default graph only.\n",
    "For example, you have two models a.py and b.py that have identical code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.90151\n",
      "-4.93518\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(2)\n",
    "c = tf.random_uniform([], -10, 10)\n",
    "d = tf.random_uniform([], -10, 10)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without graph level seed, running python a.py and b.py will return 2 completely different results, but with tf.set_random_seed, you will get two identical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.68877\n",
      "2.22114\n"
     ]
    }
   ],
   "source": [
    "%run a.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.92643\n",
      "6.14358\n"
     ]
    }
   ],
   "source": [
    "%run b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data in TensorFlow\n",
    "----\n",
    "There are two main ways to load data into a TensorFlow graph: one is through feed_dict that we are familiar with, and another is through readers that allow us to read tensors directly from file. There is, of course, the third way which is to load in your data using constants, but you should only use this if you want your graph to be seriously bloated and un-runnable (I made up another\n",
    "word but you know what I mean).\n",
    "To see why we need something more than feed_dict, we need to look into how feed_dict works under the hood. Feed_dict will first send data from the storage system to the client, and then from client to the worker process. This will cause the data to slow down, especially if the client is on a different machine from the worker process. TensorFlow has readers that allow us to load data directly into the worker process.\n",
    "The improvement will not be noticeable when we aren’t on a distributed system or when our dataset is small, but it’s still something worth looking into. TensorFlow has several built in readers to match your reading needs.\n",
    "```python\n",
    "tf.TextLineReader\n",
    "Outputs the lines of a file delimited by newlines\n",
    "E.g. text files, CSV files\n",
    "\n",
    "tf.FixedLengthRecordReader\n",
    "Outputs the entire file when all files have same fixed lengths\n",
    "E.g. each MNIST file has 28 x 28 pixels, CIFAR-10 32 x 32 x 3\n",
    "\n",
    "tf.WholeFileReader\n",
    "Outputs the entire file content\n",
    "\n",
    "tf.TFRecordReader\n",
    "Reads samples from TensorFlow's own binary format (TFRecord)\n",
    "\n",
    "tf.ReaderBase\n",
    "Allows you to create your own readers\n",
    "```\n",
    "\n",
    "Run [word2vec.py](https://github.com/AppleFairy/CS20SI-Tensorflow-for-Deep-Learning-Research/blob/master/word2vec.py) and check visalization result from tensorboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
