{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to structure your model in TensorFlow\n",
    "----------------\n",
    "Common steps:\n",
    "    \n",
    "    Phase 1: assemble your graph:\n",
    "        1. Define placeholders for input and output\n",
    "        2. Define the weights\n",
    "        3. Define the inference model\n",
    "        4. Define loss function\n",
    "        5. Define optimizer\n",
    "    Phase 2: execute the computation Which is basically training your model. There are a few steps:\n",
    "        1. Initialize all model variables for the first time.\n",
    "        2. Feed in the training data. Might involve randomizing the order of data samples.\n",
    "        3. Execute the inference model on the training data, so it calculates for each training input example the output with the current model parameters.\n",
    "        4. Compute the cost\n",
    "        5. Adjust the model parameters to minimize/maximize the cost depending on the model.\n",
    "        \n",
    "    Let’s apply these steps to creating our word2vec, skip-gram model(use center word predict context).\n",
    "\n",
    "\n",
    "## Phase 1: Assemble graph\n",
    "-------\n",
    "### 1. Define placeholders for input and output\n",
    "        Using word index as input instead of using one-hot vector. The index limit to vocabulary size.\n",
    "\n",
    "```python\n",
    "center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "```\n",
    "### 2. Define the weight (in this case, embedding matrix)\n",
    "```python\n",
    "embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))\n",
    "```\n",
    "### 3. Inference (compute the forward path of the graph)\n",
    "```python\n",
    "tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)\n",
    "```\n",
    "    \n",
    "    This interface help get embeedding like below:\n",
    "![embedding-lookup](pic/embedding-lookup.png)\n",
    "\n",
    "    So we can get embedding by below code:\n",
    "```python\n",
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words)\n",
    "```\n",
    "### 4. Define the loss function\n",
    "    The noise-contrastive estimation loss is defined in terms of a logistic regression model. For this, we need to define the weights and biases for each word in the vocabulary (also called the output weights as opposed to the input embeddings).\n",
    "```python\n",
    "nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / EMBED_SIZE ** 0.5))\n",
    "nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                     biases=nce_bias,\n",
    "                                     labels=target_words,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=NUM_SAMPLED,\n",
    "                                     num_classes=VOCAB_SIZE))\n",
    "\n",
    "```\n",
    "### 5. Define optimizer\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "```\n",
    "## Phase 2: Execute the computation\n",
    "-------\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    average_loss = 0.0\n",
    "    for index in xrange(NUM_TRAIN_STEPS):\n",
    "        batch = batch_gen.next()\n",
    "        loss_batch, _ = sess.run([loss, optimizer], feed_dict={center_words: batch[0], target_words: batch[1]})\n",
    "        average_loss += loss_batch\n",
    "        \n",
    "        if (index + 1) % 2000 == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, average_loss / (index + 1)))\n",
    " ```\n",
    "## Name Scope\n",
    "-------\n",
    "    Simplify the graph displaied by tensorboard.\n",
    "```python\n",
    "with tf.name_scope('data'):\n",
    "    center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='center_words')\n",
    "    target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1], name='target_words')\n",
    "with tf.name_scope('embed'):\n",
    "    embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name='embed_matrix')\n",
    "with tf.name_scope('loss'):\n",
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "    nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / math.sqrt(EMBED_SIZE)), name='nce_weight')\n",
    "    nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                         biases=nce_bias,\n",
    "                                         labels=target_words,\n",
    "                                         inputs=embed,\n",
    "                                         num_sampled=NUM_SAMPLED,\n",
    "                                         num_classes=VOCAB_SIZE),\n",
    "                                         name='loss')\n",
    "```\n",
    "## Tensorboard Visualization Guide\n",
    "```python\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# obtain the embedding_matrix after you’ve trained it\n",
    "final_embed_matrix = sess.run(model.embed_matrix)\n",
    "\n",
    "# create a variable to hold your embeddings. It has to be a variable. Constants\n",
    "# don’t work. You also can’t just use the embed_matrix we defined earlier for our model. Why\n",
    "# is that so? I don’t know. I get the 500 most popular words.\n",
    "embedding_var = tf.Variable(final_embed_matrix[:500], name='embedding')\n",
    "sess.run(embedding_var.initializer)\n",
    "config = projector.ProjectorConfig()\n",
    "summary_writer = tf.summary.FileWriter(LOGDIR)\n",
    "\n",
    "# add embeddings to config\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# link the embeddings to their metadata file. In this case, the file that contains\n",
    "# the 500 most popular words in our vocabulary\n",
    "embedding.metadata_path = LOGDIR + '/vocab_500.tsv'\n",
    "\n",
    "# save a configuration file that TensorBoard will read during startup\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "# save our embedding\n",
    "saver_embed = tf.train.Saver([embedding_var])\n",
    "saver_embed.save(sess, LOGDIR + '/skip-gram.ckpt', 1)\n",
    "```\n",
    "\n",
    "## Code\n",
    "----\n",
    "Below code is simple to help understand word2vec and tensorflow interfaces. [word2vec.py](https://github.com/AppleFairy/CS20SI-Tensorflow-for-Deep-Learning-Research/blob/master/word2vec.py) is better structered version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size=17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5237, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "average loss=268.66864013671875\n",
      "average loss=113.91653091955185\n",
      "average loss=52.352816532373431\n",
      "average loss=32.642002570152286\n",
      "average loss=23.842516065001487\n",
      "average loss=17.989490259289742\n",
      "average loss=14.263883960723877\n",
      "average loss=11.648746370553971\n",
      "average loss=10.119616622447968\n",
      "average loss=8.5096043688058849\n",
      "average loss=8.1126635189056397\n",
      "average loss=6.9557174841165539\n",
      "average loss=6.9018532980680467\n",
      "average loss=6.6658235789537432\n",
      "average loss=6.3691237981319428\n",
      "average loss=6.0086117869615556\n",
      "average loss=6.0014327297210697\n",
      "average loss=5.6934737144708629\n",
      "average loss=5.7688532098531722\n",
      "average loss=5.5262235835790632\n",
      "average loss=5.267850191831589\n",
      "average loss=5.3348670673370364\n",
      "average loss=5.2499580035209652\n",
      "average loss=5.212439147114754\n",
      "average loss=5.2053453102111815\n",
      "average loss=4.9726408501863482\n",
      "average loss=5.0341244877576825\n",
      "average loss=5.1856546976566316\n",
      "average loss=5.0388384627103804\n",
      "average loss=5.0516923855543139\n",
      "average loss=4.9578266538381577\n",
      "average loss=4.9890397877693173\n",
      "average loss=4.8415690518617627\n",
      "average loss=4.5940732535123825\n",
      "average loss=4.9812348027229305\n",
      "average loss=4.9003083223104476\n",
      "average loss=4.7578450702428814\n",
      "average loss=4.7858481942415239\n",
      "average loss=4.7418213547468184\n",
      "average loss=4.8144393680691717\n",
      "average loss=4.7792324092388156\n",
      "average loss=4.761090139508247\n",
      "average loss=4.7476067281961445\n",
      "average loss=4.768371035814285\n",
      "average loss=4.7530817486047745\n",
      "average loss=4.7258690967559813\n",
      "average loss=4.6738660531044003\n",
      "average loss=4.7312705062627796\n",
      "average loss=4.6825955811738966\n",
      "average loss=4.5905042814016346\n",
      "average loss=4.71184550011158\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# MACRO\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128\n",
    "EPOCH = 100001\n",
    "SKIP_WINDOW = 1 # the number of context words from left/right of input word\n",
    "NUM_SKIPS = 2    # the number of labels used for one input\n",
    "NUM_SAMPLED = 64\n",
    "\n",
    "# data\n",
    "data_name = \"data/text/text8.zip\"\n",
    "\n",
    "def read_data():\n",
    "    with zipfile.ZipFile(data_name)    as zf:\n",
    "        data = tf.compat.as_str(zf.read(zf.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "words = read_data()\n",
    "print(\"data size=%r\" % len(words))\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    #temp = collections.Counter(words))\n",
    "    count.extend(collections.Counter(words).most_common(VOCAB_SIZE - 1))\n",
    "    vocabulary = dict()\n",
    "\n",
    "    for word, _ in count:\n",
    "        vocabulary[word] = len(vocabulary) # index\n",
    "\n",
    "    indices = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            index = vocabulary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        indices.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reversed_vocabulary = dict(zip(vocabulary.values(), vocabulary.keys()))\n",
    "    return indices, count, vocabulary, reversed_vocabulary\n",
    "\n",
    "indices, count, vocabulary, reversed_vocabulary = build_dataset(words)\n",
    "\n",
    "del vocabulary\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', indices[:10], [reversed_vocabulary[i] for i in indices[:10]])\n",
    "\n",
    "index = 0\n",
    "def generate_batch():\n",
    "    assert BATCH_SIZE % NUM_SKIPS == 0\n",
    "    assert NUM_SKIPS <=    (2 * SKIP_WINDOW)\n",
    "    batch = np.ndarray(shape=(BATCH_SIZE), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(BATCH_SIZE, 1), dtype=np.int32)\n",
    "    span = 2 * SKIP_WINDOW + 1\n",
    "    buf = collections.deque(maxlen=span)\n",
    "\n",
    "    global index\n",
    "    # round back\n",
    "    if index + span > len(indices):\n",
    "        index = 0\n",
    "\n",
    "    buf.extend(indices[index:index + span])\n",
    "    index += span\n",
    "\n",
    "    for i in range(BATCH_SIZE // NUM_SKIPS): # for each span\n",
    "        target = SKIP_WINDOW # center words as target\n",
    "        targets_to_avoid = [SKIP_WINDOW]\n",
    "        \n",
    "        for j in range(NUM_SKIPS):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * NUM_SKIPS + j] = buf[SKIP_WINDOW]\n",
    "            labels[i * NUM_SKIPS + j, 0] = buf[target]\n",
    "        \n",
    "        if index == len(indices):\n",
    "            buf[:] = indices[:span]\n",
    "            index = span\n",
    "        else:\n",
    "            buf.append(indices[index])\n",
    "            index += 1\n",
    "\n",
    "    index = (index + len(indices) - span) % len(indices)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "# skip-gram model\n",
    "# define placeholder for input and output\n",
    "train_inputs = tf.placeholder(tf.int32, [BATCH_SIZE])\n",
    "train_labels = tf.placeholder(tf.int32,[BATCH_SIZE, 1])\n",
    "\n",
    "# define the weight\n",
    "embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))\n",
    "\n",
    "# inference\n",
    "embed = tf.nn.embedding_lookup(embed_matrix, train_inputs)\n",
    "\n",
    "# define the loss function\n",
    "nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / EMBED_SIZE ** 0.5))\n",
    "nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                    biases=nce_bias, \n",
    "                                    labels=train_labels, \n",
    "                                    inputs=embed,\n",
    "                                    num_sampled=NUM_SAMPLED,\n",
    "                                    num_classes=VOCAB_SIZE))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    average_loss = 0.0\n",
    "    for step in range(EPOCH):\n",
    "        batch_inputs, batch_labels = generate_batch()\n",
    "        feed_dict = {train_inputs:batch_inputs, train_labels:batch_labels}\n",
    "        _, batch_loss = sess.run([optimizer, loss], feed_dict)\n",
    "        average_loss += batch_loss\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            print(\"average loss=%r\" % average_loss)\n",
    "            average_loss = 0    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
