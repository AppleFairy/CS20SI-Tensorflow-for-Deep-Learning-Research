{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with convolutions in TensorFlow\n",
    "TensorFlow has great support for convolutional layers. The most popular one is tf.nn.conv2d.\n",
    "```python\n",
    "tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "Input: Batch size x Height x Width x Channels\n",
    "Filter: Height x Width x Input Channels x Output Channels (e.g. [5, 5, 3, 64])\n",
    "Strides: 4 element 1-D tensor, strides in each direction (often [1, 1, 1, 1] or [1, 2, 2, 1])\n",
    "Padding: 'SAME' or 'VALID'\n",
    "Data_format: default to NHWC\n",
    "```\n",
    "![convolution operation effect](./pic/convolution.png)\n",
    "There are also several other built-in convolutional operations.\n",
    "## Convnet on MNIST\n",
    "-----\n",
    "For MNIST, we will be using two convolutional layers, each followed by a relu and a maxpool layers, and one fully connected layer. \n",
    "![mnist convolution version structure](./pic/mnist_convolution_version.png)\n",
    "### Variable scope\n",
    "Since we’ll be dealing with multiple layers, it’s important to introduce variable scope. Think of a variable scope something similar to a namespace. A variable name ‘weights’ in variable scope ‘conv1’ will become ‘conv1-weights’. The common practice is to create a variable scope for each layer, so that if you have variable ‘weights’ in both convolution layer 1 and convolution layer 2, there won’t be any name clash.\n",
    "In variable scope, we don’t create variable using tf.Variable, but instead use tf.get_variable()\n",
    "```python\n",
    "tf.get_variable(<name>, <shape>, <initializer>)\n",
    "```\n",
    "If a variable with that name already exists in that variable scope, we use that variable. If a variable with that name doesn’t already exists in that variable scope, TensorFlow creates a new variable. This setup makes it really easy to share variables across architecture. This will come in extremely handy when you build complex models and you need to share large sets of variables. Variable scopes help you initialize all of them in one place.\n",
    "Nodes in the same variable scope will be grouped together, and therefore you don’t have to use name scope any more. To declare a variable scope, you do it the same way you do name scope:\n",
    "```python\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "```\n",
    "For example:\n",
    "```python\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    w = tf.get_variable('weights', [5, 5, 1, 32])\n",
    "    b = tf.get_variable('biases', [32], initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(images, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv + b, name=scope.name)\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    w = tf.get_variable('weights', [5, 5, 32, 64])\n",
    "    b = tf.get_variable('biases', [64], initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(conv1, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + b, name=scope.name)\n",
    "```\n",
    "\n",
    " Please refer to the [official documentation](https://www.tensorflow.org/api_docs/) for more information.\n",
    " \n",
    " ## Code\n",
    " ----\n",
    "Below code is simple to help understand convolution interfaces of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-90389af53187>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_batches=%r test_num=%r loss=%r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ziaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ziaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ziaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ziaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ziaz\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "MNIST = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "\n",
    "\n",
    "# input shape defaults to NHWC(batch_size, height, width, channel)\n",
    "# kernel shape defaults to (height, width, input_channel, output_channel)\n",
    "def generate_filter_layer(layer_name, input_layer, kernel_shape, bias_shape, stride_shape=[1, 1, 1, 1], padding='SAME'):\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        k = tf.get_variable('kernel', kernel_shape)\n",
    "        b = tf.get_variable('biases', bias_shape, initializer=tf.random_normal_initializer())\n",
    "        conv = tf.nn.conv2d(input_layer, k, strides=stride_shape, padding=padding)\n",
    "        return tf.nn.relu(conv + b, name=scope.name)\n",
    "\n",
    "def generate_pool_layer(layer_name, input_layer, ksize, stride_shape=[1, 2, 2, 1], padding='SAME'):\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        return tf.nn.max_pool(input_layer, ksize=ksize, strides=stride_shape, padding=padding)\n",
    "\n",
    "def generate_fc_layer(layer_name, input_layer, input_features):\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w = tf.get_variable('weights', [input_features, 1024],\n",
    "                            initializer=tf.truncated_normal_initializer())\n",
    "        b = tf.get_variable('biases', [1024],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # reshape pool2 to 2 dimensional\n",
    "        input_layer = tf.reshape(input_layer, [-1, input_features])\n",
    "        return tf.nn.relu(tf.matmul(input_layer, w) + b, name='relu')\n",
    "\n",
    "with tf.name_scope(\"data\"):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"input\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"lables\")\n",
    "\n",
    "conv1 = generate_filter_layer(layer_name='conv1',\n",
    "                            input_layer=tf.reshape(X, shape=[-1, 28, 28, 1]), # -1: dynamically decided \n",
    "                            kernel_shape=[5, 5, 1, 32],\n",
    "                            bias_shape=[32])\n",
    "pool1 = generate_pool_layer(layer_name='pool1', input_layer=conv1, ksize=[1, 2, 2, 1])\n",
    "conv2 = generate_filter_layer(layer_name='conv2',\n",
    "                            input_layer=pool1,\n",
    "                            kernel_shape=[5, 5, 32, 64],\n",
    "                            bias_shape=[64])\n",
    "pool2 = generate_pool_layer(layer_name='pool2', input_layer=conv2, ksize=[1, 2, 2, 1])\n",
    "fc = generate_fc_layer(layer_name='fc', input_layer=pool2, input_features = 7 * 7 * 64)\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    w = tf.get_variable('weights', [1024, 10],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [10],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    logits = tf.matmul(fc, w) + b\n",
    "with tf.name_scope('loss'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "            _, loss_r = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch})\n",
    "\n",
    "    print(\"n_batches=%r test_num=%r loss=%r\" % (n_batches, MNIST.train.num_examples, loss_r))\n",
    "    \n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch})\n",
    "    print(\"total_correct_preds=%r Accuracy=%r\" % (total_correct_preds, total_correct_preds/MNIST.test.num_examples))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
