{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with convolutions in TensorFlow\n",
    "TensorFlow has great support for convolutional layers. The most popular one is tf.nn.conv2d.\n",
    "```python\n",
    "tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "Input: Batch size x Height x Width x Channels\n",
    "Filter: Height x Width x Input Channels x Output Channels (e.g. [5, 5, 3, 64])\n",
    "Strides: 4 element 1-D tensor, strides in each direction (often [1, 1, 1, 1] or [1, 2, 2, 1])\n",
    "Padding: 'SAME' or 'VALID'\n",
    "Data_format: default to NHWC\n",
    "```\n",
    "![convolution operation effect](./pic/convolution.png)\n",
    "There are also several other built-in convolutional operations.\n",
    "## Convnet on MNIST\n",
    "-----\n",
    "For MNIST, we will be using two convolutional layers, each followed by a relu and a maxpool layers, and one fully connected layer. \n",
    "![mnist convolution version structure](./pic/mnist_convolution_version.png)\n",
    "### Variable scope\n",
    "Since we’ll be dealing with multiple layers, it’s important to introduce variable scope. Think of a variable scope something similar to a namespace. A variable name ‘weights’ in variable scope ‘conv1’ will become ‘conv1-weights’. The common practice is to create a variable scope for each layer, so that if you have variable ‘weights’ in both convolution layer 1 and convolution layer 2, there won’t be any name clash.\n",
    "In variable scope, we don’t create variable using tf.Variable, but instead use tf.get_variable()\n",
    "```python\n",
    "tf.get_variable(<name>, <shape>, <initializer>)\n",
    "```\n",
    "If a variable with that name already exists in that variable scope, we use that variable. If a variable with that name doesn’t already exists in that variable scope, TensorFlow creates a new variable. This setup makes it really easy to share variables across architecture. This will come in extremely handy when you build complex models and you need to share large sets of variables. Variable scopes help you initialize all of them in one place.\n",
    "Nodes in the same variable scope will be grouped together, and therefore you don’t have to use name scope any more. To declare a variable scope, you do it the same way you do name scope:\n",
    "```python\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "```\n",
    "For example:\n",
    "```python\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    w = tf.get_variable('weights', [5, 5, 1, 32])\n",
    "    b = tf.get_variable('biases', [32], initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(images, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv + b, name=scope.name)\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    w = tf.get_variable('weights', [5, 5, 32, 64])\n",
    "    b = tf.get_variable('biases', [64], initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(conv1, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + b, name=scope.name)\n",
    "```\n",
    "\n",
    " Please refer to the [official documentation](https://www.tensorflow.org/api_docs/) for more information.\n",
    " \n",
    " ## Code\n",
    " ----\n",
    "Below code is simple to help understand convolution interfaces of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "MNIST = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "\n",
    "\n",
    "# input shape defaults to NHWC(batch_size, height, width, channel)\n",
    "# kernel shape defaults to (height, width, input_channel, output_channel)\n",
    "def generate_filter_layer(layer_name, input_layer, kernel_shape, bias_shape, stride_shape=[1, 1, 1, 1], padding='SAME'):\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        k = tf.get_variable('kernel', kernel_shape)\n",
    "        b = tf.get_variable('biases', bias_shape, initializer=tf.random_normal_initializer())\n",
    "        conv = tf.nn.conv2d(input_layer, k, strides=stride_shape, padding=padding)\n",
    "        return tf.nn.relu(conv + b, name=scope.name)\n",
    "\n",
    "def generate_pool_layer(layer_name, input_layer, ksize, stride_shape=[1, 2, 2, 1], padding='SAME'):\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        return tf.nn.max_pool(input_layer, ksize=ksize, strides=stride_shape, padding=padding)\n",
    "\n",
    "def generate_fc_layer(layer_name, input_layer, input_features):\n",
    "    with tf.variable_scope(layer_name) as scope:\n",
    "        w = tf.get_variable('weights', [input_features, 1024],\n",
    "                            initializer=tf.truncated_normal_initializer())\n",
    "        b = tf.get_variable('biases', [1024],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # reshape pool2 to 2 dimensional\n",
    "        input_layer = tf.reshape(input_layer, [-1, input_features])\n",
    "        return tf.nn.relu(tf.matmul(input_layer, w) + b, name='relu')\n",
    "\n",
    "with tf.name_scope(\"data\"):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"input\")\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"lables\")\n",
    "\n",
    "conv1 = generate_filter_layer(layer_name='conv1',\n",
    "                            input_layer=tf.reshape(X, shape=[-1, 28, 28, 1]), # -1: dynamically decided \n",
    "                            kernel_shape=[5, 5, 1, 32],\n",
    "                            bias_shape=[32])\n",
    "pool1 = generate_pool_layer(layer_name='pool1', input_layer=conv1, ksize=[1, 2, 2, 1])\n",
    "conv2 = generate_filter_layer(layer_name='conv2',\n",
    "                            input_layer=pool1,\n",
    "                            kernel_shape=[5, 5, 32, 64],\n",
    "                            bias_shape=[64])\n",
    "pool2 = generate_pool_layer(layer_name='pool2', input_layer=conv2, ksize=[1, 2, 2, 1])\n",
    "fc = generate_fc_layer(layer_name='fc', input_layer=pool2, input_features = 7 * 7 * 64)\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    w = tf.get_variable('weights', [1024, 10],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [10],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    logits = tf.matmul(fc, w) + b\n",
    "with tf.name_scope('loss'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "            _, loss_r = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch})\n",
    "\n",
    "    print(\"n_batches=%r test_num=%r loss=%r\" % (n_batches, MNIST.train.num_examples, loss_r))\n",
    "    \n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch})\n",
    "    print(\"total_correct_preds=%r Accuracy=%r\" % (total_correct_preds, total_correct_preds/MNIST.test.num_examples))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
