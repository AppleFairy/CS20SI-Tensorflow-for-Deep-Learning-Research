{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline\n",
    "-----\n",
    " In input pipeline, multiple threats can help us reduce the bottleneck at the reading in data phase because reading in data is a lot of waiting. For example, in using queues to prepare inputs for training a model, we have:\n",
    "* Multiple threads prepare training examples and push them in the queue.\n",
    "* A training thread executes a training op that dequeues mini-batches from the queue\n",
    "\n",
    "Three concepts for multi-thread programming in Tensorflow\n",
    "* Queue: threads operate queues.\n",
    "* QueueRunner: the wrapper of threads.\n",
    "* Coordinator: coordinate all threads.\n",
    "Please check [threading and queues](https://www.tensorflow.org/programmers_guide/threading_and_queues) for details.\n",
    "All related interfaces can be found from [inputs and readers](https://www.tensorflow.org/api_guides/python/io_ops).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "NUM_THREADS = 4\n",
    "\n",
    "# Generating some simple data\n",
    "# create 1000 random samples, each is a 1D array from the normal distribution (10, 1)\n",
    "data = 10 * np.random.randn(N_SAMPLES, 4) + 1\n",
    "\n",
    "# create 1000 random labels of 0 and 1\n",
    "target = np.random.randint(0, 2, size=N_SAMPLES)\n",
    "queue = tf.FIFOQueue(capacity=50, dtypes=[tf.float32, tf.int32], shapes=[[4], []])\n",
    "enqueue_op = queue.enqueue_many([data, target])\n",
    "dequeue_op = queue.dequeue()\n",
    "\n",
    "# create NUM_THREADS to do enqueue\n",
    "qr = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\n",
    "with tf.Session() as sess:\n",
    "    # Create a coordinator, launch the queue runner threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    enqueue_threads = qr.create_threads(sess, coord=coord, start=True)\n",
    "    for step in range(100): # do to 100 iterations\n",
    "        if coord.should_stop():\n",
    "            break\n",
    "        data_batch, label_batch = sess.run(dequeue_op)\n",
    "        #print(data_batch)\n",
    "        #print(label_batch)\n",
    "    coord.request_stop()\n",
    "    coord.join(enqueue_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also don’t need to use tf.Coordinator with TensorFlow queues, but can use it to manage threads of any thread you create. For example, you use the Python package threading to create threads to do some crazy job, you can still use tf.Coordinator to manage these threads too. The syntax of target and args are similar to the classic threadpool. For more details on threading, you\n",
    "should take CS 110. The example below is from TensorFlow documentation.\n",
    "```python\n",
    "import threading\n",
    "# thread body: loop until the coordinator indicates a stop was requested.\n",
    "# if some condition becomes true, ask the coordinator to stop.\n",
    "def my_loop(coord):\n",
    "    while not coord.should_stop():\n",
    "        ...do something...\n",
    "    if ...some condition...:\n",
    "        coord.request_stop()\n",
    "        \n",
    "# main code: create a coordinator.\n",
    "coord = tf.Coordinator()\n",
    "\n",
    "# create 10 threads that run 'my_loop()'\n",
    "# you can also create threads using QueueRunner as the example above\n",
    "threads = [threading.Thread(target=my_loop, args=(coord,)) for _ in xrange(10)]\n",
    "\n",
    "# start the threads and wait for all of them to stop.\n",
    "for t in threads:\n",
    "    t.start()\n",
    "coord.join(threads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Readers\n",
    "----\n",
    "We have talked about this title in [manage experiments](https://github.com/AppleFairy/CS20SI-Tensorflow-for-Deep-Learning-Research/blob/master/manage-experiments.ipynb)\n",
    "```python\n",
    "tf.TextLineReader\n",
    "Outputs the lines of a file delimited by newlines\n",
    "E.g. text files, CSV files\n",
    "\n",
    "tf.FixedLengthRecordReader\n",
    "Outputs the entire file when all files have same fixed lengths\n",
    "E.g. each MNIST file has 28 x 28 pixels, CIFAR-10 32 x 32 x 3\n",
    "\n",
    "tf.WholeFileReader\n",
    "Outputs the entire file content. This is useful when each file contains a sample\n",
    "\n",
    "tf.TFRecordReader\n",
    "Reads samples from TensorFlow's own binary format (TFRecord)\n",
    "    \n",
    "tf.ReaderBase\n",
    "Allows you to create your own readers\n",
    "```\n",
    "To use data reader, we first need to create a queue to hold the names of all the files you want to read in through tf.train.string_input_producer, it creates a FIFOQueue under the hood, so to run the queue, we’ll need tf.Coordinator and tf.QueueRunner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(\"data/heart.csv\")\n",
    "reader = tf.TextLineReader(skip_header_lines=1) # skip the first line in the file\n",
    "key, value = reader.read(filename_queue)\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    print(sess.run(key)) # data/heart.csv:2\n",
    "    print(sess.run(value)) # 144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below line of code above will parse value into the tensor record defaults which we have to create ourselves. The record defaults serve two purposes:\n",
    "* First, it tells the decoder what types of data to expect in each column.\n",
    "* Second, if a space in a column happens to be empty, it’ll fill in that space with the default value of the data type that we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    content = tf.decode_csv(value, record_defaults=record_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the record_defaults of this specific dataset, we’d like it to have 10 elements. All elements are either integers or floats, except for the fifth element that is a string. To make it easier, we assume\n",
    "that all feature integers are floats (we’ll still specify the 10th column to be integer, because we like our labels to be integer).\n",
    "```python\n",
    "record_defaults = [[1.0] for _ in range(N_FEATURES)] # define all features to be floats\n",
    "record_defaults[4] = [''] # make the fifth feature string\n",
    "record_defaults.append([1])\n",
    "content = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "```\n",
    "You can also do all the kind of pre-processing you need for your data before feeding it in. For example, now we have our content is a list of 10 elements, 8 are floats, 1 is string, and 1 is integer. We’ll have to convert the string to float (Absent as 0 and Present as 1), and then convert the first 9 features into a tensor that can be fed into the model.\n",
    "```python\n",
    "# convert the 5th column (present/absent) to the binary value 0 and 1\n",
    "condition = tf.equal(content[4], tf.constant('Present'))\n",
    "content[4] = tf.select(condition, tf.constant(1.0), tf.constant(0.0))\n",
    "# pack all 9 features into a tensor\n",
    "features = tf.pack(content[:N_FEATURES])\n",
    "# assign the last column to label\n",
    "label = content[-1]\n",
    "```\n",
    "With that, every time the reader reads in a line from our CSV file, it’ll convert that line into a feature tensor and a label!\n",
    "But we often don’t want to feed in a single sample into our model, but instead, we would want to batch ‘em up. You can do so using tf.train.batch, or tf.train.shuffle_batch if you want to shuffle your batches.\n",
    "```python\n",
    "# minimum number elements in the queue after a dequeue, used to ensure\n",
    "# that the samples are sufficiently mixed\n",
    "# I think 10 times the BATCH_SIZE is sufficient\n",
    "min_after_dequeue = 10 * BATCH_SIZE\n",
    "# the maximum number of elements in the queue\n",
    "capacity = 20 * BATCH_SIZE\n",
    "# shuffle the data to generate BATCH_SIZE sample pairs\n",
    "data_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "```\n",
    "And with that we’re done. You can simply use data_batch and label_batch the way you would have used input_placeholder and label_placeholder in our previous model, except you don’t need to feed them in through the feed_dict parameters.\n",
    "## TFRecord\n",
    "----\n",
    "Like many machine learning frameworks, TensorFlow has its own binary data format which is called TFRecord. A TFRecord is a serialized tf.train.Example Protobuf object. They can be created in a few lines of code. Below is an example to convert an image into a TFRecord.\n",
    "First, we need to read in the image and convert it to byte string.\n",
    "```python\n",
    "def get_image_binary(filename):\n",
    "    image = Image.open(filename)\n",
    "    image = np.asarray(image, np.uint8)\n",
    "    shape = np.array(image.shape, np.int32)\n",
    "    return shape.tobytes(), image.tobytes() # convert image to raw data bytes in the array.\n",
    "```\n",
    "Next, you write these byte strings into a TFRecord file using tf.python_io.TFRecordWriter and tf.train.Features. You need the shape information so you can reconstruct the image from the binary format later.\n",
    "```python\n",
    "def write_to_tfrecord(label, shape, binary_image, tfrecord_file):\n",
    "    \"\"\" This example is to write a sample to TFRecord file. If you want to write\n",
    "    more samples, just use a loop.\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(tfrecord_file)\n",
    "    # write label, shape, and image content to the TFRecord file\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'label': tf.train.Feature(bytes_list=tf.train.BytesList(value=[label])),\n",
    "            'shape': tf.train.Feature(bytes_list=tf.train.BytesList(value=[shape])),\n",
    "            'image':tf.train.Feature(bytes_list=tf.train.BytesList(value=[binary_image]))\n",
    "            }))\n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "```\n",
    "To read a TFRecord file, you use TFRecordReader and tf.decode_raw.\n",
    "```python\n",
    "def read_from_tfrecord(filenames):\n",
    "    tfrecord_file_queue = tf.train.string_input_producer(filenames, name='queue')\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, tfrecord_serialized = reader.read(tfrecord_file_queue)\n",
    "    # label and image are stored as bytes but could be stored as\n",
    "    # int64 or float64 values in a serialized tf.Example protobuf.\n",
    "    tfrecord_features = tf.parse_single_example(tfrecord_serialized,\n",
    "                        features={\n",
    "                        'label': tf.FixedLenFeature([], tf.string),\n",
    "                        'shape': tf.FixedLenFeature([], tf.string),\n",
    "                        'image': tf.FixedLenFeature([], tf.string),\n",
    "                        }, name='features')\n",
    "    # image was saved as uint8, so we have to decode as uint8.\n",
    "    image = tf.decode_raw(tfrecord_features['image'], tf.uint8)\n",
    "    shape = tf.decode_raw(tfrecord_features['shape'], tf.int32)\n",
    "    # the image tensor is flattened out, so we have to reconstruct the shape\n",
    "    image = tf.reshape(image, shape)\n",
    "    label = tf.cast(tfrecord_features['label'], tf.string)\n",
    "    return label, shape, image\n",
    "```\n",
    "Keep in mind that label, shape, and image returned are tensor objects. To get their values, you’ll\n",
    "have to eval them in tf.Session().\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
